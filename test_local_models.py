"""
æœ¬åœ°æ¨¡å‹å¿«é€ŸéªŒè¯è„šæœ¬
éªŒè¯æœ¬åœ°æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸å·¥ä½œï¼Œä¸ºå®éªŒåšå‡†å¤‡
"""

import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from src.che.agents.ollama_agent import OllamaAgent
from src.che.core.task import Task
from src.che.core.ecosystem import Ecosystem
from src.che.prompts import PromptType, get_prompt

def test_local_models():
    """æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠŸèƒ½"""
    print("å¼€å§‹æœ¬åœ°æ¨¡å‹åŠŸèƒ½æµ‹è¯•...")
    print("="*50)
    
    # æµ‹è¯•ä»»åŠ¡
    test_task = Task(
        instruction="ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œç”¨ä¸€å¥è¯å³å¯ã€‚",
        false_premise="ä¸å­˜åœ¨çš„è™šå‡å‰æ"  # ä½¿ç”¨éç©ºçš„è™šå‡å‰æ
    )
    
    # åˆ›å»ºä¸åŒç±»å‹çš„æ™ºèƒ½ä½“è¿›è¡Œæµ‹è¯•
    agents_to_test = [
        {
            "id": "test_critical",
            "model": "gemma:2b",
            "prompt_type": PromptType.CRITICAL,
            "role": "critical"
        },
        {
            "id": "test_standard", 
            "model": "qwen:7b-chat",
            "prompt_type": PromptType.STANDARD,
            "role": "standard"
        },
        {
            "id": "test_awakened",
            "model": "llama3:latest", 
            "prompt_type": PromptType.AWAKENED,
            "role": "awakened"
        }
    ]
    
    results = {}
    
    for agent_config in agents_to_test:
        print(f"\næµ‹è¯•æ™ºèƒ½ä½“: {agent_config['id']} (æ¨¡å‹: {agent_config['model']}, è§’è‰²: {agent_config['role']})")
        
        try:
            # åˆ›å»ºæ™ºèƒ½ä½“
            agent = OllamaAgent(
                agent_id=agent_config['id'],
                config={
                    "model": agent_config['model'],
                    "prompt": get_prompt(agent_config['prompt_type'])
                }
            )
            
            # æµ‹è¯•æ™ºèƒ½ä½“å“åº”
            response = agent.execute(test_task)
            print(f"  âœ“ å“åº”æˆåŠŸ: {response[:100]}...")
            results[agent_config['id']] = {
                "status": "success",
                "response_preview": response[:100],
                "model": agent_config['model'],
                "role": agent_config['role']
            }

        except Exception as e:
            print(f"  âœ— å“åº”å¤±è´¥: {str(e)}")
            results[agent_config['id']] = {
                "status": "error",
                "error": str(e),
                "model": agent_config['model'],
                "role": agent_config['role']
            }
    
    return results

def test_ecosystem():
    """æµ‹è¯•ç”Ÿæ€ç³»ç»ŸåŠŸèƒ½"""
    print(f"\nå¼€å§‹ç”Ÿæ€ç³»ç»ŸåŠŸèƒ½æµ‹è¯•...")
    print("="*50)
    
    try:
        # åˆ›å»ºä¸€ä¸ªå°å‹ç”Ÿæ€ç³»ç»Ÿè¿›è¡Œæµ‹è¯•
        ecosystem = Ecosystem()
        
        # æ·»åŠ æµ‹è¯•æ™ºèƒ½ä½“
        test_agents = [
            OllamaAgent(
                agent_id="eco_test_1",
                config={
                    "model": "gemma:2b",
                    "prompt": get_prompt(PromptType.CRITICAL)
                }
            ),
            OllamaAgent(
                agent_id="eco_test_2", 
                config={
                    "model": "qwen:7b-chat",
                    "prompt": get_prompt(PromptType.STANDARD)
                }
            )
        ]
        
        for agent in test_agents:
            ecosystem.add_agent(agent)
        
        print(f"  âœ“ æˆåŠŸåˆ›å»ºåŒ…å« {len(ecosystem.agents)} ä¸ªæ™ºèƒ½ä½“çš„ç”Ÿæ€ç³»ç»Ÿ")
        
        # æµ‹è¯•ä»»åŠ¡æ‰§è¡Œ
        test_task = Task(
            instruction="ç®€å•è®¡ç®—ï¼š2+2ç­‰äºå¤šå°‘ï¼Ÿ",
            false_premise="ä¸å­˜åœ¨çš„è™šå‡å‰æ"
        )
        
        scores = ecosystem.run_generation(test_task)
        print(f"  âœ“ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼Œè·å¾— {len(scores)} ä¸ªå“åº”")
        
        for agent_id, score in scores.items():
            print(f"    - {agent_id}: å¾—åˆ† {score}")
        
        return {
            "status": "success",
            "agent_count": len(ecosystem.agents),
            "response_count": len(scores)
        }
        
    except Exception as e:
        print(f"  âœ— ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

def main():
    """ä¸»å‡½æ•°"""
    print("è®¤çŸ¥å¼‚è´¨æ€§å®éªŒ - æœ¬åœ°æ¨¡å‹éªŒè¯")
    print(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    # æ‰§è¡Œæ¨¡å‹æµ‹è¯•
    model_results = test_local_models()
    
    # æ‰§è¡Œç”Ÿæ€ç³»ç»Ÿæµ‹è¯•
    ecosystem_result = test_ecosystem()
    
    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    validation_report = {
        "timestamp": datetime.now().isoformat(),
        "model_tests": model_results,
        "ecosystem_test": ecosystem_result,
        "overall_status": "success" if all(r['status'] == 'success' for r in model_results.values()) and ecosystem_result['status'] == 'success' else "partial_success"
    }
    
    # ä¿å­˜éªŒè¯æŠ¥å‘Š
    report_filename = f"local_model_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_filename, 'w', encoding='utf-8') as f:
        json.dump(validation_report, f, ensure_ascii=False, indent=2)
    
    print(f"\néªŒè¯æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_filename}")
    
    # æ€»ç»“
    print(f"\néªŒè¯æ€»ç»“:")
    print(f"- æ™ºèƒ½ä½“æ¨¡å‹æµ‹è¯•: {'é€šè¿‡' if all(r['status'] == 'success' for r in model_results.values()) else 'éƒ¨åˆ†é€šè¿‡'}")
    print(f"- ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•: {'é€šè¿‡' if ecosystem_result['status'] == 'success' else 'å¤±è´¥'}")
    print(f"- æ•´ä½“çŠ¶æ€: {validation_report['overall_status']}")
    
    if validation_report['overall_status'] == 'success':
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è¿è¡Œå®éªŒã€‚")
        print(f"å»ºè®®è¿è¡Œå‘½ä»¤: python -c \"from src.che.experimental.cognitive_independence_experiment import run_evolution_experiment; run_evolution_experiment(generations=2, population_size=6)\"")
    else:
        print(f"\nâš ï¸  æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œè¯·æ£€æŸ¥é”™è¯¯å¹¶è§£å†³åå†è¿è¡Œå®éªŒã€‚")
    
    return validation_report

if __name__ == "__main__":
    main()